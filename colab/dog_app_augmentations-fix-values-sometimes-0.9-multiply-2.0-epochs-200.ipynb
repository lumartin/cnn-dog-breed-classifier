{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"dog_app_augmentations-fix-values-sometimes-0.9-multiply-2.0-epochs-200.ipynb","provenance":[{"file_id":"1CB8ottEi6O0V1byMpFFwPGAsDASBjZMB","timestamp":1586083613524},{"file_id":"1Z8yuY-OvTPNR_I39ExXUOetQgZCd5ai3","timestamp":1586083560447},{"file_id":"1CCtPCrvTO1eJTvBKDYBNh1qzbLulOgIB","timestamp":1586083192152},{"file_id":"1XEMq3W--Egp-IwHfHLUl8fooafAISsSq","timestamp":1586046109632},{"file_id":"1M6NnHno6mb6UmPwY3rLfVYIgx4r300qj","timestamp":1586045971599},{"file_id":"1nfhbSpamn1nTIQ94KZMjeAJvJQuUvQWH","timestamp":1586019294461},{"file_id":"1S5b4ZZBlAQH0-QI2rR3Klay6plHwlSG4","timestamp":1586019186582},{"file_id":"1x43TFVwxZ-RIkjvpxoh231Lvtt-ikN2C","timestamp":1586019059898},{"file_id":"18_yL6W-Q1P8P1xqcCJPCNjg_8OeZTD69","timestamp":1586005625719},{"file_id":"1r2_wvotCDm3k_r_TLNHFwiyqgwyV5KkQ","timestamp":1586005597098},{"file_id":"1x17ANn3WW9PJckaEQSlkio5gn7dkPN-k","timestamp":1586005574311},{"file_id":"1wrfznjw3XLiV6JfDZxRvB0FBT5wBDZ0I","timestamp":1586005511862},{"file_id":"1dOYwkMe0ADYqIypoTsfACJ0m1aC2wFkJ","timestamp":1585991244277},{"file_id":"1f9SevfjtTDJD6CO-HfNW-osiYvgrGJvN","timestamp":1585991217369},{"file_id":"1XWPqI_yXRmz4PenEhf7W2irFoa6HT6uF","timestamp":1585991195574},{"file_id":"1XNOEqHACPONOBz8ZN9L-aSYsuM0I2xJd","timestamp":1585991128475},{"file_id":"1E3y3-KvBsy9UapPVrGZypZT00YexjZbR","timestamp":1585955348971},{"file_id":"116DV5OYEC78n8Tdpt9Q7_MzL_TdLq6Qe","timestamp":1585955020650},{"file_id":"14GYMrkofK2E3cxEXO0Wa3eUO1nV6TOXr","timestamp":1585941706806},{"file_id":"1vcnE6D3-w6WNefC-orZ7W0KpV6HOmgnW","timestamp":1585781564679},{"file_id":"1CuKotTscUY4BCBXthHtT6SvrXJtC5f2I","timestamp":1585484980455},{"file_id":"1q5wJQ8TKeGV62hAt3Qj8gTaCOJkqUNgd","timestamp":1585323933479}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JXxOTx7DU-o4","colab_type":"code","outputId":"7598a2da-e605-4c63-bf35-0578baf5db9a","executionInfo":{"status":"ok","timestamp":1586131125105,"user_tz":-120,"elapsed":1269,"user":{"displayName":"Luis Martín","photoUrl":"","userId":"05136960784416304431"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qkVY9R0tYZw3","colab_type":"code","colab":{}},"source":["%%capture\n","\n","!wget -N https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n","\n","!unzip -o /content/dogImages.zip\n","\n","!mv /content/dogImages /content/drive/My\\ Drive/dog-breeds/dog_images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbuFvy-OAzXp","colab_type":"code","outputId":"e8d4d9f9-0733-4035-b301-0a05854ff61e","executionInfo":{"status":"ok","timestamp":1586132545614,"user_tz":-120,"elapsed":3666,"user":{"displayName":"Luis Martín","photoUrl":"","userId":"05136960784416304431"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","from glob import glob\n","\n","experiment_description = 'sometimes_0.9_multiply_2.0'\n","\n","\n","project_path = '/content/drive/My Drive/dog-breeds/'\n","\n","experiment_location = project_path + experiment_description\n","\n","escaped_exp_location = experiment_location.replace(\" \", \"\\\\ \")\n","!mkdir $escaped_exp_location\n","\n","model_file = experiment_location + '/model_ ' + experiment_description\n","output_file = experiment_location + '/' + experiment_description\n","\n","\n","dog_files = np.array(glob(project_path + \"dog_images/*/*/*\"))\n","\n","print('There are %d total dog images.' % len(dog_files))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["There are 8351 total dog images.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GM3cs3LyNyRI","colab_type":"code","outputId":"d6ec0a43-c8ad-4972-bb36-663f144aac96","executionInfo":{"status":"ok","timestamp":1586165216584,"user_tz":-120,"elapsed":14900,"user":{"displayName":"Luis Martín","photoUrl":"","userId":"05136960784416304431"}},"colab":{"base_uri":"https://localhost:8080/","height":905,"output_embedded_package_id":"1ZGI-1HMnI4m_Vg0Cjlb1HwgwJnkMevoP"}},"source":["#!pip install git+https://github.com/aleju/imgaug\n","from imgaug import augmenters as iaa\n","import imgaug as ia\n","import PIL\n","import numpy as np\n","import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import matplotlib as mpl\n","mpl.rcParams['axes.grid'] = False\n","mpl.rcParams['image.interpolation'] = 'nearest'\n","mpl.rcParams['figure.figsize'] = 15, 25\n","\n","def show_dataset(dataset, n=6):\n","  img = np.vstack((np.hstack((np.asarray(dataset[i][0]) for _ in range(n)))\n","                   for i in range(n)))\n","  plt.imshow(img)\n","  plt.axis('off')\n","  \n","class ImgAugTransform:\n","  def __init__(self):\n","\n","    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n","\n","    self.aug = iaa.Sequential([\n","        iaa.Resize((224, 224)),\n","        #iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n","        #iaa.OneOf([iaa.GaussianBlur((0, 3.0)),iaa.AverageBlur(k=(2, 7)),iaa.MedianBlur(k=(3, 11)),]),\n","        #sometimes(iaa.Superpixels(p_replace=(0, 1.0),n_segments=(20, 200))),\n","        #iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n","        #iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n","        #sometimes(iaa.OneOf([iaa.EdgeDetect(\n","        #          alpha=(0, 0.7)),iaa.DirectedEdgeDetect(\n","        #          alpha=(0, 0.7), direction=(0.0, 1.0)),])),\n","        #iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n","        #iaa.Fliplr(0.5),\n","        sometimes(iaa.Crop(percent=(0, 0.45))),\n","        #iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n","        #iaa.Invert(0.05, per_channel=True),\n","        #iaa.Add((-10, 10), per_channel=0.5),\n","        iaa.Multiply((0.5, 2.5), per_channel=0.5),\n","        #iaa.LinearContrast((0.5, 2.0), per_channel=0.5),\n","        iaa.Grayscale(alpha=(0.0, 1.0)),\n","        #sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)),\n","        #iaa.Sometimes(0.25,iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n","        #                              iaa.CoarseDropout(0.1, \n","        #                              size_percent=0.5)])),\n","        #iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True),\n","        #sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05)))\n","    ])\n","      \n","  def __call__(self, img):\n","    img = np.array(img)\n","    return self.aug.augment_image(img)\n","\n","transforms = ImgAugTransform()\n","\n","dataset = torchvision.datasets.ImageFolder(project_path + 'dog_images/train', transform=transforms)\n","\n","show_dataset(dataset, 6)"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"_7Dz5_TRtpjr","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","\n","def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path, verbose=False):\n","    \"\"\"returns trained model\"\"\"\n","    \n","    valid_loss_min = np.Inf \n","    initial_time = time.time()\n","    train_losses, valid_losses = [], []\n","    \n","    for epoch in range(1, n_epochs+1):\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","        initial_epoch_time = time.time()\n","\n","        model.train()\n","        for batch_idx, (data, target) in enumerate(loaders['train']):\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","            optimizer.zero_grad()\n","            log_ps = model(data)\n","            loss = criterion(log_ps, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","\n","        model.eval()\n","        with torch.no_grad():\n","            for batch_idx, (data, target) in enumerate(loaders['valid']):\n","                if use_cuda:\n","                    data, target = data.cuda(), target.cuda()\n","\n","                ps_log = model(data)\n","                loss = criterion(ps_log, target)\n","                valid_loss = valid_loss + (1 / (batch_idx + 1)) * (loss.data - valid_loss)\n","\n","            if verbose:\n","                final_epoch_time = time.time()\n","                print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTime spent: {:.6f}'.format(\n","                   epoch, \n","                   train_loss,\n","                   valid_loss,\n","                   final_epoch_time - initial_epoch_time\n","                   ))            \n","        \n","        train_losses.append(train_loss)\n","        valid_losses.append(valid_loss)\n","        \n","        if valid_loss <= valid_loss_min:\n","           torch.save(model.state_dict(), save_path)\n","           valid_loss_min = valid_loss    \n","    \n","    final_time = time.time()\n","    return model, train_losses, valid_losses, final_time - initial_time\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ksid6BWZJlFO","colab_type":"code","colab":{}},"source":["%%capture cap --no-stderr\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","\n","import os\n","import torch\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from PIL import ImageFile\n","import matplotlib.pyplot as plt\n","import datetime\n","import torch.nn as nn\n","import torch.nn.functional as F\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","\n","mixed_transforms = torchvision.transforms.Compose([\n","    ImgAugTransform(),\n","    lambda x: PIL.Image.fromarray(x),\n","    torchvision.transforms.RandomVerticalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","valid_transforms = transforms.Compose([transforms.Resize(size=224),\n","                                           transforms.CenterCrop(224),\n","                                           transforms.ToTensor(),\n","                                           transforms.Normalize([0.485, 0.456, 0.406],\n","                                                            [0.229, 0.224, 0.225])])\n","\n","\n","\n","train_data = datasets.ImageFolder(project_path + 'dog_images/train', transform=mixed_transforms)\n","valid_data = datasets.ImageFolder(project_path + 'dog_images/valid', transform=valid_transforms)\n","test_data = datasets.ImageFolder(project_path + 'dog_images/test', transform=valid_transforms)\n","\n","\n","trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=0)\n","validloader = torch.utils.data.DataLoader(valid_data, batch_size=32, shuffle=True, num_workers=0)\n","testloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=0)\n","\n","loaders_scratch = {'train' : trainloader, 'valid' : validloader, 'test' : testloader}\n","\n","\n","class BasicBlock(nn.Module):\n","    \"\"\"Basic Block for resnet 18 and resnet 34\n","    \"\"\"\n","\n","    #BasicBlock and BottleNeck block \n","    #have different output size\n","    #we use class attribute expansion\n","    #to distinct\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","\n","        #residual function\n","        self.residual_function = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n","        )\n","\n","        #shortcut\n","        self.shortcut = nn.Sequential()\n","\n","        #the shortcut output dimension is not the same with residual function\n","        #use 1*1 convolution to match the dimension\n","        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n","            )\n","        \n","    def forward(self, x):\n","        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n","\n","class BottleNeck(nn.Module):\n","    \"\"\"Residual block for resnet over 50 layers\n","    \"\"\"\n","    expansion = 4\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.residual_function = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n","            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n","        )\n","\n","        self.shortcut = nn.Sequential()\n","\n","        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n","                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n","            )\n","        \n","    def forward(self, x):\n","        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n","    \n","class ResNet(nn.Module):\n","\n","    def __init__(self, block, num_block, num_classes=133):\n","        super().__init__()\n","\n","        self.in_channels = 64\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True))\n","        #we use a different inputsize than the original paper\n","        #so conv2_x's stride is 1\n","        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n","        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n","        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n","        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","    def _make_layer(self, block, out_channels, num_blocks, stride):\n","        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the \n","        same as a neuron netowork layer, ex. conv layer), one layer may \n","        contain more than one residual block \n","        Args:\n","            block: block type, basic block or bottle neck block\n","            out_channels: output depth channel number of this layer\n","            num_blocks: how many blocks per layer\n","            stride: the stride of the first block of this layer\n","        \n","        Return:\n","            return a resnet layer\n","        \"\"\"\n","\n","        # we have num_block blocks per layer, the first block \n","        # could be 1 or 2, other blocks would always be 1\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_channels, out_channels, stride))\n","            self.in_channels = out_channels * block.expansion\n","        \n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        output = self.conv1(x)\n","        output = self.conv2_x(output)\n","        output = self.conv3_x(output)\n","        output = self.conv4_x(output)\n","        output = self.conv5_x(output)\n","        output = self.avg_pool(output)\n","        output = output.view(output.size(0), -1)\n","        output = self.fc(output)\n","\n","        return output \n","\n","def resnet18():\n","    \"\"\" return a ResNet 18 object\n","    \"\"\"\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","\n","\n","model_scratch = resnet18()\n","print (model_scratch)\n","\n","use_cuda = torch.cuda.is_available()\n","if use_cuda:\n","    model_scratch.cuda()\n","\n","learning_rates = [\n","                 #0.001\n","                 0.0015,\n","                 0.0025,\n","                 #0.0035\n","\n","                 ]\n","\n","optimizers = [\n","                #optim.Adagrad,\n","                optim.Adam\n","            ]\n","\n","\n","def test(loaders, model, criterion, use_cuda):\n","    test_loss = 0.\n","    correct = 0.\n","    total = 0.\n","\n","    model.eval()\n","    for batch_idx, (data, target) in enumerate(loaders['test']):\n","        if use_cuda:\n","            data, target = data.cuda(), target.cuda()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n","        total += data.size(0)\n","    print('Test Loss: {:.6f}'.format(test_loss))\n","    print('\\nTest Accuracy: %2d%% (%2d/%2d)\\n\\n' % (\n","        100. * correct / total, correct, total))\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None:\n","            torch.nn.init.zeros_(m.bias)\n","\n","\n","\n","def reboot(loaders, model, criterion):\n","    gpu_info = !nvidia-smi\n","    gpu_info = '\\n'.join(gpu_info)\n","    if gpu_info.find('failed') >= 0:\n","      print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n","      print('and then re-execute this cell.')\n","    else:\n","      print(gpu_info)    \n","\n","    del loaders\n","    del model\n","    del criterion\n","    torch.cuda.empty_cache()\n","    \n","    trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=4)\n","    validloader = torch.utils.data.DataLoader(valid_data, batch_size=32, shuffle=True, num_workers=4)\n","    testloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=4)\n","    \n","    new_model = resnet18()\n","    new_model.apply(weights_init)\n","    new_model.cuda()\n","    \n","    new_criterion = criterion_scratch = nn.CrossEntropyLoss()\n","    \n","    return {'train' : trainloader, 'valid' : validloader, 'test' : testloader}, new_model, new_criterion\n","\n","current_optimizer = ''\n","model_scratch = resnet18()\n","criterion_scratch = ''\n","for optimizer in optimizers:\n","    for lr in learning_rates:\n","        loaders_scratch, model_scratch, criterion_scratch = reboot(loaders_scratch, model_scratch, criterion_scratch)\n","        del current_optimizer\n","        current_optimizer = optimizer(model_scratch.parameters(), lr)\n","        \n","        print(current_optimizer)\n","        model, train_losses, valid_losses, time_spent = train(100, loaders_scratch, \n","                                                              model_scratch, \n","                                                              current_optimizer, \n","                                                              criterion_scratch, \n","                                                              use_cuda, \n","                                                              model_file + '_lr_' + str(lr) + '.pt',\n","                                                              verbose=True)\n","        \n","        print(\"time spent: \", str(datetime.timedelta(seconds=time_spent)))\n","        \n","        min_train_loss = float(min(train_losses))\n","        min_valid_loss = float(min(valid_losses))\n","        \n","        print(\"Minimum trainig loss: \", min_train_loss)\n","        print(\"Minimum validation loss: \", min_valid_loss)\n","\n","        plt.plot(train_losses, label='Training loss')\n","        plt.plot(valid_losses, label='Validation loss')\n","        \n","        plt.legend(frameon=False)\n","        plt.show()\n","       \n","        model_scratch.load_state_dict(torch.load(model_file + '_lr_' + str(lr) + '.pt'))\n","        test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)\n","\n","        with open(output_file + '_lr_' + str(lr) + '.txt', 'w') as f:\n","          f.write(cap.stdout)\n","        plt.savefig(output_file + '_lr_' + str(lr) + '.png')       \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIajAq0W3g08","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"8a7548c2-842e-4c24-f012-0e54d46efc98","executionInfo":{"status":"ok","timestamp":1586165165308,"user_tz":-120,"elapsed":11951,"user":{"displayName":"Luis Martín","photoUrl":"","userId":"05136960784416304431"}}},"source":["model_scratch.load_state_dict(torch.load(model_file + '_lr_' + str(lr) + '.pt'))\n","test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Test Loss: 1.940372\n","\n","Test Accuracy: 49% (414/836)\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g0mdAlOrzI_s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f050e01d-34dd-475d-9645-8e2d0d3a787d","executionInfo":{"status":"ok","timestamp":1586164992996,"user_tz":-120,"elapsed":985,"user":{"displayName":"Luis Martín","photoUrl":"","userId":"05136960784416304431"}}},"source":["print(model_file + '_lr_' + str(lr) + '.pt')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/dog-breeds/sometimes_0.9_multiply_2.0/model_ sometimes_0.9_multiply_2.0_lr_0.0025.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"f039feca-b4e8-4cfd-9638-67a29cba87bd","executionInfo":{"status":"ok","timestamp":1586165118447,"user_tz":-120,"elapsed":13879,"user":{"displayName":"Luis Martín","photoUrl":"","userId":"05136960784416304431"}},"id":"HNO_X8kNz3VP","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["model_scratch.load_state_dict(torch.load('/content/drive/My Drive/dog-breeds/sometimes_0.9_multiply_0.5/model_ sometimes_0.9_multiply_0.5_lr_0.0015.pt'))\n","test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test Loss: 6.319849\n","\n","Test Accuracy:  8% (73/836)\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NmNNsKpjzZPH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}